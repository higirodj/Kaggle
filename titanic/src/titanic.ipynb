{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "960bf2d7-70ed-44f7-9ee2-2463862684de",
    "_uuid": "5c6792727d3f7619b94b1ef2bfc629f379867077"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer, Imputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "TITANIC_PATH = \"/Users/JuliusHigiro/Machine-Learning/titanic/datasets/\"\n",
    "\n",
    "def load_titanic_data(dataset_path=TITANIC_PATH):\n",
    "    csv_training_path = os.path.join(dataset_path, \"train.csv\")\n",
    "    csv_test_path = os.path.join(dataset_path, \"test.csv\")\n",
    "    return pd.read_csv(csv_training_path), pd.read_csv(csv_test_path)\n",
    "            \n",
    "def prefix(name):\n",
    "    return name[name.find(\",\"):name.find(\".\")].split(\",\")[1].strip()\n",
    "\n",
    "def age_range(age):\n",
    "    if age > 60: return 'elderly'\n",
    "    elif 39 < age <= 60: return 'adult'\n",
    "    elif 18 < age <= 39: return 'young adult'\n",
    "    elif age <= 18: return 'child'\n",
    "    else: return 'unknown' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "57640eb4-960c-4e37-b5b0-f3bcd63a7b90",
    "_uuid": "bf2dbda7972b6cf4eedd5fd92a3ae329948e4add"
   },
   "source": [
    "# A. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "03561a6f-f5a2-400a-9f19-852052c53950",
    "_uuid": "d333cdfcfb5bb5515045feaab2c0a7bb15430302"
   },
   "source": [
    "1. **Objective:** Predict the survival rate of passengers on the Titanic with the highest accuracy.\n",
    "<br>\n",
    "<br>\n",
    "2. **Method:** \n",
    "    1. Explore the data and obtain insights and useful information for developing prediction models.\n",
    "    2. Use information learned from the data exploration to create useful features that will enhance our models.\n",
    "    3. Process and prepare the data for the various machine learning models that we explored in the project.\n",
    "    4. Learn a prediction model on the dataset using several different classifiers.\n",
    "<br>\n",
    "<br>\n",
    "3. **Data:** The dataset consists of 891 entries with some entries missing feature values.\n",
    "\n",
    "    1. survival\n",
    "    2. pclass\n",
    "    3. sex\n",
    "    4. Age\n",
    "    5. sibsp\n",
    "    6. parch\n",
    "    7. ticket\n",
    "    8. fare\n",
    "    9. cabin\n",
    "    10. embarked\n",
    "<br>\n",
    "<br>\n",
    "4. **Results:** Our highest submission score was 0.78947."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " training_data, test_data = load_titanic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c1c7f88-6b83-4a8d-b21b-10d2a04c3e7d",
    "_uuid": "e89d41b98b67ab8c68cd8b70b98b5cdb87f90f70"
   },
   "source": [
    "# B. EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f601f456-531a-49e5-88d5-80831f6485f4",
    "_uuid": "d5ebcff25f2f3a826f6e3f56c12f393333c7888d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize the different predictors vs the target values\n",
    "\n",
    "fig, axes = subplots(3)\n",
    "fig.set_size_inches(10, 25)\n",
    "j = 0;\n",
    "\n",
    "predictors = array(['Pclass', 'SibSp', 'Parch'])\n",
    "\n",
    "for predictor in predictors:\n",
    "    values = unique(df[predictor])\n",
    "    survived = numpy.zeros(len(values))\n",
    "    totals = numpy.zeros(len(values))\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(values)):\n",
    "        survived[i] = sum(training_data['Survived'][training_data[predictor] == values[i]] == 1)\n",
    "        totals[i] = len(training_data['Survived'][training_data[predictor] == values[i]])\n",
    "    sca(axes[j])\n",
    "    \n",
    "    bar(arange(len(values)), survived/totals)\n",
    "    xticks(arange(len(values)), values)\n",
    "    \n",
    "    axes[j].set_title( 'Survival Rate by ' + predictor)\n",
    "    \n",
    "    j+= 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eea9f14b-9284-4a51-acf1-6dd8512575cf",
    "_uuid": "09fbf241914eec011da23aa6f0c6c3e0b39889ea"
   },
   "source": [
    "The graph titled 'Survival Rate by Pclass' shows us the percentage of each of the different Pclasses that survived. It is clear that a passenger's Pclass impacted their survival rate. Pclass 1 had the highest, followed by Pclass 2, and then Pclass 3, suggesting that passengers with higher socio-economic status had a better chance of survival.\n",
    "   \n",
    "\n",
    "Subgraphs relating to SibSp and Parch also show that having a smaller family unit improved survival. If a passenger traveled with more than four childern and parents, then their chances of survival dropped significantly. This might suggest that passengers with many children or parents gave up their seat on a life boat for their chilren/parents. The same results are reflected in the SibSp chart; traveling with less than two siblings and spouses greatly increased your chance of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ffc5e96a-8dfd-4d83-98c2-eaa8bfcaef05",
    "_uuid": "6946c1be47ac73217ea4056f395bb2ce12cab8f3"
   },
   "outputs": [],
   "source": [
    "# a look at the embarked data\n",
    "sns.factorplot(x=\"Embarked\", kind=\"count\", size=8, data=training_data)\n",
    "sns.factorplot(x=\"Embarked\", hue=\"Survived\", col=\"Survived\", kind=\"count\", size=8, data=training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e2491248-1f43-4cf4-8a97-196ec79f27aa",
    "_uuid": "8fe420b8c9f575d488ae8a9c806838dcd49716b0"
   },
   "source": [
    "By examining the 'Embarked' variable we can see that most of the passengers boarded at Southampton. Since more people boarded at that location, it isn't surprising that it had the greatest number of survivors. When we examine the three locations by survival percentage (See below), it is clear that Cherbourg had the highest percentage of survivors. This could be explained by the class of the guest that boarded at each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e547bd34-2f79-4dea-8fc4-7888735c5b64",
    "_uuid": "10067fdf11a228503c092e63179c765bb54ac833"
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"Embarked\", y=\"Survived\", kind=\"bar\", size=8, data=training_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7d6e81b1-2016-47b6-9fb9-781636cd4a23",
    "_uuid": "ee1f4ba8f2fea1c99b724933f78c03d0948521fe"
   },
   "source": [
    "The graph below shows the distribution of Pclass that boarded at each location. As we guessed, most of the passengers that boarded at Cherbourg belonged to a higher socio-economic class, specifically Pclass 1. We already know that Pclass 1 has the highest survival rate among the different Pclasses, so this helps to explain why Cherbourg had such a high survival percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3db30a79-9a09-4629-a5ea-6a1c53f6df2b",
    "_uuid": "6b883367899579b9031f74b9b0f8100bfa105753"
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"Embarked\", hue = 'Pclass', kind=\"count\", size=8, data=training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8433a21a-b5ca-4b72-bf93-a332b4ced606",
    "_uuid": "b9dd1051f4376f7d7c0351f3b176eeca2459c718"
   },
   "outputs": [],
   "source": [
    "Below we can see graphs that display both the total number that survived and the survival percentage categorized by sex. It is clear there were more female passengers that survived and that the percentage of females that survived is much higher than the percentage of males that survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5468c14d-1baf-47e7-bc14-c15fbb6a61b7",
    "_uuid": "b21288f5967d590ef0f7b5eae6558197c4dcea18"
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"Sex\", hue=\"Survived\", col=\"Survived\", kind=\"count\", size=8, data=training_data)\n",
    "sns.factorplot(x=\"Sex\", y=\"Survived\", kind=\"bar\", size=6, data=training_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dfd66c03-48a5-4e8d-bcee-933dde1acff2",
    "_uuid": "b9f3be36bde89770d9a3fd92384eff75c63fc5ea"
   },
   "source": [
    "We can also examine the survival rate of the sexes by their Pclass. As expected, females have a higher rate of surival than the males in the same Pclass. It's interesting that see that a female in Pclass 3 has a better change of surival than a male in Pclass 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7ad1a99d-cbc4-42c2-8c3c-7dabe5ba5b44",
    "_uuid": "dc277a39d47f6c564d9b56db981ca47807f913de"
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"bar\", size=8, data=training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8dc0ae7f-cb1b-46e7-b9bb-e42099c926e9",
    "_uuid": "8dec22cb8e3377d240f1a068ec93bff4c1d87730"
   },
   "source": [
    "We will now take a look at the survival rate based on the age of the passenger.For this we group each passenger into one of five age groups - child, young adult, adult, elderly, or unknown. If a passenger is missing their age value we put them in the unknown group for now. These age groups are used only for the visualizations, not as actual predictors. In our model we use the exact age of each passenger as the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "06ad8365-43b4-4530-8a6e-77bf441fb4bb",
    "_uuid": "d7cb06370e1663b90efc935c57c803349ce8445c"
   },
   "outputs": [],
   "source": [
    "training_data['Age_Groups'] = training_data.Age.map(age_range)\n",
    "sns.factorplot(x=\"Age_Groups\", hue=\"Survived\", kind=\"count\", size = 8, data=training_data)\n",
    "\n",
    "sns.factorplot(x=\"Age_Groups\", y=\"Survived\", kind=\"bar\", size = 8, data=training_data)\n",
    "\n",
    "sns.factorplot(x=\"Age_Groups\", hue=\"Survived\", col=\"Sex\", kind=\"count\", aspect=0.5, size = 8, data=training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "22d22be7-919e-4a9a-9c14-42487e72d0c1",
    "_uuid": "2d788112ed836a75d5fc5f44851b7a46ca5eae40"
   },
   "source": [
    "As you can see above, children had the highest survival rate among all age groups. When you consider gender as well you see that more females survived than males in each age group. It seems the saying of \"women and children first\" holds true for the titanic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3135f589-173b-4d4a-99e4-56b11112f33e",
    "_uuid": "6b5a8d46be04c2f8a33c55116e8784b29cef1755"
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"Age_Groups\", y=\"Survived\", col=\"Pclass\", kind=\"bar\", aspect=0.5, size = 8, data=training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fadd834b-c443-4ac5-8499-efed53e89218",
    "_uuid": "f94ec294c3f47fa43a30a6d0dd784f118a79f4c4"
   },
   "source": [
    "If we examine each age group by their Pclass, we see that higher socio-economic groups tend to have higher survival rates than their counterpart in lower socio-economic groups. This reinforces our above findings. We also note that children have the highest survival rate out of every Pclass group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "83a94397-5788-4f0c-b8c0-dfd0fa598bfc",
    "_uuid": "9b09c2970e36ed0f52e419ea22c5ad81f85e79ff"
   },
   "source": [
    "Finally, we examine the survival rate by Fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "efb5528d-d903-4a16-bec9-ae7e09dac5cd",
    "_uuid": "57ddf882c55a575fa9e028fb9173b331e8d4e3ad"
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"Survived\", y=\"Fare\", hue=\"Sex\", col=\"Sex\", kind=\"bar\", size = 8, data=training_data)\n",
    "sns.factorplot(x=\"Pclass\", y=\"Fare\", col = \"Survived\", kind=\"box\", size = 8, data=training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f78bae3e-6a89-4833-9212-12e2c8c1259b",
    "_uuid": "5e79c636fdeee960e03be02925ba5172b9ae267d"
   },
   "source": [
    "It's interesting to note that passengers that paid a higher fare amount tended to have a higher rate of survival. This could have to do with the 'Cabin' group. Customers that payed more for their fare might have ended up in cabins closer to the stairs or to the life boats. This could help them get to the boats faster, which would increase their survival rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c3cf4903-903a-4467-ba23-e9ffab2f3968",
    "_uuid": "c45373870b4ac5ca2082b79940f1cb625b2debd6"
   },
   "source": [
    "# C. FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "50c1ac71-81d1-4a88-9a0b-da2433ca51dc",
    "_uuid": "8dd0597e47b76e82a7f151464117ac8b6ff7a381"
   },
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0ccd967-fe7f-46ac-9c0e-764bb7ad96c3",
    "_uuid": "68c44861a4b8411acd6e45b26208380f57502cd1"
   },
   "source": [
    "Now that we have read in our data, we can begin to consider some new features. At first glance the 'Name' column seems useless; we can't just use it as a predictor since almost every passenger will have a unique name. However we can extract the title from each passenger and add this into our models. This might give us even more ensight into the socio-economic status of a passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e1a8b364-755d-425a-a3d7-51aa4d28328d",
    "_uuid": "7d09fdfd98d759a8f89a7d8af4376bf7bdf8216c"
   },
   "outputs": [],
   "source": [
    "training_data[\"Title\"] = training_data.Name.map(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fa3389c9-90e5-4569-91de-d4095248f02b",
    "_uuid": "c135c7165fc993247f404157d5cc660856f21105"
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"Title\", y=\"Survived\", kind=\"bar\", size=8, aspect = 2, data=training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a92dff5e-804a-4df3-8f4b-706968ef9b10",
    "_uuid": "7d167ea1a0c21af45ff7127066bff4c13c8b9138"
   },
   "source": [
    "As you can see above, classes that are associated with higher socio-economic status, like Lady, Sir, or  the Countess, have a higher survival rate. This graph also reinforces the data showing that females are more likely to survive than males. Titles associated with women, like Mrs, and Ms, have a much higher survival rate than the male counterpart Mr and Master. This graph can may be misleading since some titles are associated with very few passengers. To avoid overfitting we will group upper socio-economic male titles into a group called 'sir' and the upper socio-economic females titles into a group called 'Lady'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {\n",
    "                \"Don\":        \"Sir\",\n",
    "                \"Sir\":        \"Sir\",\n",
    "                \"Col\":        \"Sir\",\n",
    "                \"Dr\":         \"Sir\",\n",
    "                \"Capt\":       \"Sir\",\n",
    "                \"Major\":      \"sir\",\n",
    "                \"Rev\":        \"Sir\",\n",
    "                \"Jonkheer\":   \"Sir\",\n",
    "                \"Lady\" :      \"Lady\",\n",
    "                \"the Countess\":\"Lady\",\n",
    "                \"Mrs\" :       \"Lady\",\n",
    "                \"Mme\":        \"Lady\",\n",
    "                \"Mlle\":       \"Lady\",\n",
    "                \"Ms\":         \"Lady\",\n",
    "                \"Mr\" :        \"Mr\",\n",
    "                \"Miss\" :      \"Miss\",\n",
    "                \"Master\" :    \"Master\",\n",
    "                \"Lady\" :      \"Lady\"\n",
    "\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['Title'] = training_data.Title.map(name_dict)\n",
    "training_data['Title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9803c2d1-5baf-4ca7-922f-1461eabaa08b",
    "_uuid": "a33f312ab64e5886ebf40b7a5c95cf405f14ab7a"
   },
   "source": [
    "Below is a chart that shows the survival rate of the new groups. We can see that the 'Lady' group has the highest surival rate, which is to be expected. We also see that the 'Sir' group has a higher survival rate than the 'Mr' group. This is also expected since groups with a higher socio-economic status generally have a higher survival rate than other groups of the same gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b7c93c2a-d33a-47f1-83d5-af87b4f17530",
    "_uuid": "eb85a5bfe639ce8c8da6418dc9382a40bbe1ab19"
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.factorplot(x=\"Title\", y=\"Survived\", kind=\"bar\", size=8, data=training_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a8b7e3fc-f770-4225-b674-a6bc8859c4dc",
    "_uuid": "fc90bc9322d58ef1c94dfddc27ee0278c4467008"
   },
   "source": [
    "### Family Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "20941a25-c93c-405c-a7cf-42cdb01bbb23",
    "_uuid": "89b4b519b2a6f34d751457e245405577482a0948"
   },
   "source": [
    "We thought it might also be worthwhile to examine the size of the family each passenger was traveling with. This can be accomplished by adding the number of siblings and spouses a passenger is traveling with (SibSp) and the number of parents and children a passenger is traveling with (Parch). From the results above we expect that smaller families units will have a higher rate of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7f87c8ac-b1b7-466a-a4a3-53da92fd2e32",
    "_uuid": "0fdb04d7270cc5b39a840d65abee98a8b516edf5"
   },
   "outputs": [],
   "source": [
    "training_data[\"Family_size\"] = df.SibSp + df.Parch + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6db1547d-a798-4a6a-97d1-ea3151a741b9",
    "_uuid": "c9841a2ce976c6775d06cad69553acfb7207d1bc"
   },
   "source": [
    "The graph below displays the rate of survival for the family size each passenger is traveling with. As expected smaller families have a higher survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "28c50be1-f634-493b-ae02-e2d729ebdddd",
    "_uuid": "9718de51e4b5e1de49312557f757eae791ebcf84"
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"Family_size\", y=\"Survived\", kind=\"bar\", size=8, data=training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "811c68b6-b3eb-4915-84ca-a42eeb38424d",
    "_uuid": "67a6b1d06b882d6d53bc81e270700439304f81c5"
   },
   "source": [
    "# Preprocessing of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14c82a47-3e83-4761-b8b0-a3e00356d501",
    "_uuid": "907e277270b396b0d96144e07cffb07d6e0e8071"
   },
   "source": [
    "We decided to impute the missing values in our data. This allowed us to use passengers with missing values in our training set. Each section will discuess how we calculated the missing values for each category. After these values were calculated we converted each categorical variable, such as Title or Pclass, into several that use one hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "caf2865d-5e3b-4d0a-a05d-956a6b1103c1",
    "_uuid": "16f82f681b799e15b54136e29868232b5ddf0fa7"
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "scaler = StandardScaler()\n",
    "imputer = Imputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c8747ba0-15de-48ab-89b2-5175d216b205",
    "_uuid": "90489f300ea5bea5ea922f8a015567a4c0b373d5"
   },
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "49b5d6ac-1aa8-4198-92cc-864cd6de861a",
    "_uuid": "a014c4fa72a271f79f0f7f947fac5db1ea9ab675"
   },
   "source": [
    "We noticed that there were several passengers that were missing their age. To compute this we will use the median age of the Pclass and Title groups that the passenger belongs to. Below we can see that each Title group has a different age associated with the differnt Pclasses. By including multiple levels in this fashion, we should get a more realistic estimate on the passengers' age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3aafefdb-8d4e-4b1d-950b-31d35de1aaec",
    "_uuid": "ef63ad80b4fb47846da2459f8ba242be4583bca9"
   },
   "outputs": [],
   "source": [
    "training_data.groupby(['Pclass','Title'])['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3df0975d-bdf0-4820-8274-95361d490828",
    "_uuid": "f7745324ecb81570adf1a7714d2fefcd1e0d4b83"
   },
   "outputs": [],
   "source": [
    "training_data['Age'] = training_data.groupby(['Pclass','Title'])['Age'].transform(lambda x:x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fab6a92e-67cd-41c7-8253-50dc4b656538",
    "_uuid": "902bb5a44bb4b57c806da6ddffb83b5fc7979bab"
   },
   "source": [
    "### Embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d25974e1-8b87-4340-a3b6-605301dce6f4",
    "_uuid": "70d4cda6011a662792e703987a4d6f9316c59de6"
   },
   "source": [
    "Two passengers were missing the embarkment location. Their infomation is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f96b027f-2b66-4048-a295-57fc97c8e699",
    "_uuid": "dbcb17c54ce98bb8c0dd670a51808a841df46408"
   },
   "outputs": [],
   "source": [
    "training_data[training_data.Embarked.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9040c46e-6922-46dc-a3cd-c48d56c4c6b7",
    "_uuid": "02fc1f45f2c11de221aeb801446d4c4a4b40fc6e"
   },
   "source": [
    "We can see that both the passengers were female and belonged to Pclass 1. Below is a graph that shows the distribution of where females in Pclass 1 boarded. Since most of them boarded at Southampton, we will use it as the embarked location for the two passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8b3e3dc6-312d-4458-b896-8d17d474f462",
    "_uuid": "06f8abcc0e5bb6eadb2232380f47cf818d284f7e"
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"Embarked\", kind=\"count\", data=training_data[(training_data['Sex'] == 'female') & (df['Pclass'] == 1)], size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5ecf590d-3069-45b3-b791-2f7fc2e051b4",
    "_uuid": "4c11c47c96b31cb093cc57b9a8b98f0433ee445a"
   },
   "outputs": [],
   "source": [
    "embark_filler = 'S'\n",
    "training_data.loc[(training_data.Embarked.isnull()), 'Embarked'] = embark_filler\n",
    "\n",
    "embark = lb.fit_transform(training_data['Embarked'])\n",
    "embark_columns = lb.classes_\n",
    "embarked = pd.DataFrame(data=embark, columns=embark_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c8d8fa06-83ee-4ea9-9294-0461e157b5a1",
    "_uuid": "340e228ad7ae529b6fac94a4704060c7d5940f7a"
   },
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6dc7460-1060-4f5a-b60f-bdf3816c66d3",
    "_uuid": "4261a118c1c3b2769492bcb627146530082603c5"
   },
   "source": [
    "There were now missing values in Title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3f10365d-50de-433d-b0b3-d0d143564dd5",
    "_uuid": "5ccbb8d1477b79292447eb429f2683ea860e90f0"
   },
   "outputs": [],
   "source": [
    "title = lb.fit_transform(training_data['Title'])\n",
    "title_columns = lb.classes_\n",
    "titles = pd.DataFrame(data=title, columns=title_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9dbd4d4c-0656-4397-b07c-0a492dc1ed25",
    "_uuid": "5688db03d279075b1639e9b8576904e7253d0928"
   },
   "source": [
    "### Pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da19969a-175f-4319-94a9-f992b4118aca",
    "_uuid": "6ebb5cf74776e84c8ce7876012c1b0a46e70659f"
   },
   "source": [
    "There were no missing values in Pclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "64553fde-b6da-4983-81e4-f213a0c986de",
    "_uuid": "c4ac16329e3f859830369d52d1ef6a0def21d939"
   },
   "outputs": [],
   "source": [
    "pclass = lb.fit_transform(training_data['Pclass'])\n",
    "pclass_columns = ['Class1', 'Class2', 'Class3']\n",
    "pclasses = pd.DataFrame(data=pclass, columns=pclass_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3977a6da-edb2-404b-81b8-9a7d7e5c0a7a",
    "_uuid": "ba0cfa9196d99e7997123a4f73b13985d2746754"
   },
   "source": [
    "### Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb0ce836-f784-4b96-8b49-ee2ce7582671",
    "_uuid": "b0a7cdd822847dbf1fbcd0ee2e8cad3a114b00bd"
   },
   "source": [
    "There were no missing values in Sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "28f1dbae-4cdd-45db-b373-ce07cdd2ad02",
    "_uuid": "2fb8adff06330ed23ba9fd47009fbbb7bd719965"
   },
   "outputs": [],
   "source": [
    "sex = lb.fit_transform(training_data['Sex'])\n",
    "genders = pd.DataFrame(data=sex, columns=['Sex_transform'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "62043f7a-2bbf-4f11-805d-f835b4cd8ec2",
    "_uuid": "a600ac15d86b8b20b9ca3155d2ec8c69d33b08bb"
   },
   "source": [
    "### Combine Transformed Dataframes into Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1df06df1-72db-4490-b231-99ffc2e63d12",
    "_uuid": "91a8a386e2d8a70cc56fe52106aee90664af6a9f"
   },
   "source": [
    "Now we combine all of the one hot encoded dataframes we created above to use as testing data. We also normalize our data in this step. This allows the unit changes for each predictor to have a more equal impact on our models. If we didn't normalize the data then changes in large integers, like age, might have more of an impact than changes in smaller integers in our data, like SibSp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1709b94d-83e8-4714-b498-4ca183c4a468",
    "_uuid": "15e54d3a2761e1602cbdce3f47b2ba5e034bd8c6"
   },
   "outputs": [],
   "source": [
    "numerical_attributes=['SibSp', 'Parch', 'Age', 'Family_size', 'Fare']\n",
    "data = pd.concat([training_data[numerical_attributes], embarked, titles, pclasses, genders], axis=1)\n",
    "numerical_attributes=['SibSp', 'Parch', 'Age', 'Family_size', 'Fare']\n",
    "# scale the numerical attributes\n",
    "data[numerical_attributes] = scaler.fit_transform(data[numerical_attributes])\n",
    "targets = training_data['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c1a21db2-75bc-4a28-832c-9b9d96e00c28",
    "_uuid": "873f0ede03eafd44fcd87730d2ea8e1deba5ce92"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ddbf9255-7520-4f3f-80cd-ee4a1a95b53f",
    "_uuid": "d6b24c43ea98ae4db0bb407becc7e8cb3559489f"
   },
   "source": [
    "The first models we choose to explore were based on our findings in Orange. SVC, Random Forest, and Adaboosting had the highest accuracy rate, so these were the first models we explored. Since the classification rate wasn't satisfactory we ended up exploring more models. Each section below discusses the different models and how we tuned the parameters. A seed was set for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "73bbf970-3bcc-40f6-ab9a-80d5e7389ddc",
    "_uuid": "5c3688db9fbfc6411463938bcb7b2b8851b7cec1"
   },
   "outputs": [],
   "source": [
    "seed = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1077e65e-d23f-4636-bed9-4a605a8c3184",
    "_uuid": "a8c43c4973a19fe0de3dce6e37fb5c7515dabaee"
   },
   "source": [
    "### SVC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae86602b-bd2d-461f-ad6f-d3ea646e868b",
    "_uuid": "b7ff9e554d252a8c90f7cb4c02a6bc473e690e2a"
   },
   "source": [
    "The first model we will build will be a support vector classifier. For this model we will try to tune gamma, which is the kernel coeffecient, and C, which is the penalty parameter for the error term. When we were first tuning the gamma value we tested different magnitude values (now commented out) to find the region we should be searching. This resulted in the gamma range you see below. To find the region for the penalty parameter we followed the same procedure. We also decided to hold back to testing set to see the approximate accuracy of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fbe5acde-85aa-4c65-9e63-22340c133d07",
    "_uuid": "516ec776967589035c4fd3e68f7f4e00eb9f4785"
   },
   "outputs": [],
   "source": [
    "# determine the best SVC model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, targets, test_size=0.25, random_state=0)\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], \n",
    "                    'gamma': np.arange(1e-4, 1e-2, 1e-3),\n",
    "                     'C': [150, 175, 180, 200]}]\n",
    "\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, cv=10,\n",
    "                       scoring='accuracy')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters for our model: ')\n",
    "print(clf.best_params_)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print('\\nClassification Report for SVC Model with above parameters:')\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a3b7adb-9276-4bbe-8565-0155e50d6f89",
    "_uuid": "9d551cdca41b3f07720a3c01c6ee433909278af3"
   },
   "source": [
    "The output above shows that our model is about 81% accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1627f2ac-fbcf-4398-bbde-650679c8955f",
    "_uuid": "53b580fcb6a94459163feba0094d3b4b43f2ecb0"
   },
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a1b82559-fb25-444f-bad2-0aed90f1d449",
    "_uuid": "9e392e119cd9af427c1536f16c900ae9b505fe3f"
   },
   "source": [
    "For our random forest classifier we will be tuning n_estimators, which is the number of trees built, the criterion, the maximum features considered, the maximum depth our of trees, and whether or not the data is bootstraped. We followed the same procedure as the SVC model to determine the ranges we were searching for our parameter tuning. Again, we left out some test data to gauge the approximate accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3f45a7d6-18a6-4f65-95e3-701af3bb9646",
    "_uuid": "44d51d4f4f9a105ef7634a8a57fd9bad95d8971f"
   },
   "outputs": [],
   "source": [
    "# determine the best RandomForest model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, targets, test_size=0.5, random_state=0)\n",
    "\n",
    "tuned_parameters = [{'n_estimators': [300, 400], \n",
    "                     'criterion': ['gini', 'entropy'],\n",
    "                     'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                     'max_depth': [5, 7],\n",
    "                     'bootstrap': [True, False]}]\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=10,\n",
    "                       scoring='accuracy')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters for our model: ')\n",
    "print(clf.best_params_)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print('\\nClassification Report for Random Forest Model with above parameters:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6be9a2e9-9678-4f06-a8d7-cd8b0db5f3d2",
    "_uuid": "5e7ef391bb1e87ff76dba60cf3f82642b1203cf6"
   },
   "source": [
    "The output above shows that our model is about 82% accurate. With random forest classifiers we are also able to show the importance of our features in building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "98303089-618f-4717-bd9d-dfdccada7bd6",
    "_uuid": "443cd151aee89c1cf1edbd88d618c01d804d419c"
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 300, bootstrap = False, max_depth = 5)\n",
    "clf.fit(data, targets)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "features = np.array(['SibSp', 'Parch', 'Age', 'Family_size', 'Fare', 'C', 'Q', 'S', 'Lady', 'Master', 'Miss', 'Mr',\n",
    "                     'Mrs', 'Sir', 'Class1', 'Class2', 'Class3', 'Sex_transform'])\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), features) ## removed [indices]\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8aa73c43-2277-4278-ba4e-78ca2d683b21",
    "_uuid": "7982467516ee60f315cc318d7cb0992b01a5eff0"
   },
   "source": [
    "As we can see above, Sex seems to be the most important feature, followed by the Pclass of the passenger. This suggests that gender has the biggest influence on who survives. This reinforces the graphs above that show females survive more often than males, regardless of the passenger's Pclass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4e0a2c04-3c86-4635-9a29-d59f5e7c567d",
    "_uuid": "0d749e758eac2435bbf06d9cc30b5d3f2bec39fa"
   },
   "source": [
    "### Adaboosting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "64a6e424-2f20-41f8-a56e-8d10c936eba5",
    "_uuid": "040a63b0af6d1a7395a8f483aee2b77c4c65302f"
   },
   "source": [
    "Another model we choose to explore was the adaboosting model. For this model we choose to tune the number of estimators, the learning rate, and the algoirthm used by the model. We set aside some test data to give the approximate accuracy of the tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3432ee9f-a6b5-47bc-a6b1-72642a80f7d4",
    "_uuid": "430143c7cccb6c84d4c676365e975ad83ad34dbb"
   },
   "outputs": [],
   "source": [
    "#determine the best Adaboost model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, targets, test_size=0.5, random_state=0)\n",
    "\n",
    "tuned_parameters = [{'n_estimators': [50, 100, 150], \n",
    "                     'learning_rate': [1, .5, .25, .1],\n",
    "                     'algorithm': ['SAMME', 'SAMME.R']}]\n",
    "\n",
    "clf = GridSearchCV(AdaBoostClassifier(), tuned_parameters, cv=10,\n",
    "                       scoring='accuracy')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters for our model: ')\n",
    "print(clf.best_params_)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print('\\nClassification Report for Random Forest Model with above parameters:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6f1126c4-54ab-46e5-828a-21f86498f23c",
    "_uuid": "4eee6dd2c572a0a2804b6b40e10304eb2ca3f585"
   },
   "source": [
    "The classification report above shows that our model is approximately 79% accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8b310d36-280e-4293-9a06-f3efbfed783c",
    "_uuid": "ef556238b4e99654bee1bfdb2fd7937ef37972be"
   },
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "67989963-717c-4c38-89c6-4bbaaf9e7650",
    "_uuid": "637245ede9fda2a30722a07bae8d0402e2277bde"
   },
   "source": [
    "We also fit a gradient boosting classifier. For this model we tuned the number of boosting stages to perform, the maximum depth of the regression estimators, the minimum number of samples per leaf, the maximum number of features used when considering a split, and the learning rate. Again we set back some data to calculate the approximate accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aa26a112-0151-45b2-a678-13561c14338a",
    "_uuid": "8aa8c8d2af90a0c58a5dcdb0e4dab9bc6d758861"
   },
   "outputs": [],
   "source": [
    "# Tune parameters and fit Gradient Boosting Classifier to identify the best parameters\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.25, random_state=seed)\n",
    "\n",
    "parameters = {\n",
    "   'n_estimators': [1000, 1100],\n",
    "    'max_depth': [4, 6, 8, 10, 12],\n",
    "    'min_samples_leaf': [5, 11, 17, 21],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]   \n",
    "}\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=seed)\n",
    "gbc_gscv = GridSearchCV(estimator=gbc, cv=10, param_grid=parameters, scoring='accuracy').fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters for our model: ')\n",
    "print(gbc_gscv.best_params_)\n",
    "\n",
    "y_true, y_pred = y_test, gbc_gscv.predict(X_test)\n",
    "print('\\nClassification Report for Gradient Boosting Classifier Model with above parameters:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d26b2a6b-28c9-44d9-9d0e-772f1ee4ffc3",
    "_uuid": "09bd461c0bfa7f2cfdfb7b80afbcad7411d8fc1c"
   },
   "source": [
    "The classification report above shows that our model is approximately 79% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e80b2f5d-1cfb-4096-886d-7ddce1f8dc0b",
    "_uuid": "a717ef65e51cb6fe25692d17fa3454617ed4ebb4"
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators': 1000, 'max_depth': 6,\n",
    "          'min_samples_leaf': 5, 'learning_rate': 0.001,\n",
    "          'max_features': 'sqrt', 'random_state': seed}\n",
    "\n",
    "gbcm = GradientBoostingClassifier(**params).fit(data, targets)\n",
    "\n",
    "importances = gbcm.feature_importances_\n",
    "features = np.array(['SibSp','Parch','Fare', 'Age',\n",
    "              'Family_Size', 'Sex_transform',\n",
    "              'Class1','Class2','Class3',\n",
    "              'C','Q','S','Lady','Master','Miss',\n",
    "              'Mr','Mrs','Sir'])\n",
    "\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), features) \n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b7479312-d37f-40cb-83d3-ac812dc88b53",
    "_uuid": "f9a0bb1f2ad17a684ecb5e881ab220e820797d6f"
   },
   "source": [
    "For this model the 'Title' group was the most important feature when fitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b34972dd-5f58-4753-a519-4d73a20ea860",
    "_uuid": "8ae6320e7a4d50c3a9bc166ab32748d008977227"
   },
   "source": [
    "### DNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7bc19c40-aeea-41f0-9cf0-d1d927f9f89f",
    "_uuid": "d85e114171537e39756e3586583a63c6f87a3f99"
   },
   "source": [
    "We will also fit a DNN classifier. For this problem we will be tuning the number of nodes in each hidden layer. We chose to stick with two hidden layers. After experimenting with the magnitude of the number of nodes in each layer (3, 5, 10, 50, 100, 1000), we found that our accuracy was highest when we selected a number that is smaller than the number of predictors in our model. With that in mind we tuned our model by testing every combination of the number of nodes from one to the max number of predictors for each layer. We calculated the accuracy by setting some data back and using it as a validation set. After the model was fit we predicted values for the validation set and compared the predictions to the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d651c5b-f06e-4ba8-869b-a1fa27ab335d",
    "_uuid": "2b6f25c31371a18def7ef404cebd341a4eb3167c"
   },
   "outputs": [],
   "source": [
    "X_train = data.iloc[:600, :].astype(np.float32)\n",
    "y_train = targets.iloc[:600].astype(np.float32)\n",
    "X_test = data.iloc[600:, :].astype(np.float32)\n",
    "y_test = targets.iloc[600:].astype(np.float32)\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column('x', shape=[18])]\n",
    "\n",
    "accuracies = np.zeros((17, 17))\n",
    "\n",
    "\n",
    "for i in range(17):\n",
    "    for j in range(17):\n",
    "        \n",
    "        classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                                  hidden_units=[i+16, j+16],\n",
    "                                                  n_classes=2,\n",
    "                                                  optimizer=tf.train.AdamOptimizer())\n",
    "\n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "              x={\"x\": np.array(X_train)},\n",
    "              y=np.array(y_train),\n",
    "              num_epochs=None,\n",
    "              shuffle=True)\n",
    "\n",
    "        classifier.train(input_fn=train_input_fn, steps=5000)\n",
    "\n",
    "        test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "              x={\"x\": np.array(X_test)},\n",
    "              y=np.array(y_test),\n",
    "             num_epochs=1,\n",
    "              shuffle=False)\n",
    "\n",
    "        accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "        accuracies[i, j] = accuracy_score\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f20b71d-e7f6-43ba-955b-3d1d4e1fe916",
    "_uuid": "197773aa4ed222597a2a25471d5488d32fbd4e4d"
   },
   "source": [
    "Below is a heat map of the approximate accuracy of the different combinations of nodes per layer. You can see that most values are cloase to .8. The highest accuracy rate (.85) seems to be at 3 nodes for the first layer and 2 nodes in the second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7738229e-4862-41a1-854c-3ca70032bbd4",
    "_uuid": "b055d5308a6eb941b02fe57ba3997cce5e9b8f54"
   },
   "outputs": [],
   "source": [
    "accuracies = pd.DataFrame(accuracies)\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "ax = sns.heatmap(data=accuracies, cmap=\"YlGnBu\", annot=True, linewidths=.5, cbar_kws={'label': 'Accuracy on Test Set'})\n",
    "ax.set(xlabel='# of 2nd hidden layer nodes', ylabel='# of 1st hidden layer nodes')\n",
    "ax.set_xticklabels(np.arange(1, 18))\n",
    "ax.set_yticklabels(reversed(np.arange(1, 18)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3efcd52c-e9f7-43f8-ab97-40ce3ae15d0f",
    "_uuid": "9410490c9caaa75a655b073ea8ebfffe7af3b866"
   },
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "867ce313-3e42-43e1-add2-07879c1af609",
    "_uuid": "41826b73ff3c7bdc54b72b40414e73f0bb545a00"
   },
   "source": [
    "Since we have created so many models, we thought it would be worth looking into ways to combine the best models. Below is a graph that shows the approximate accuracy of each model. The three models with the highest accuracy rates are RFC, SVC, and GBC. We will use these three models going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fc0a65c3-6754-4c72-a720-2fd854308a0e",
    "_uuid": "c84431402e114d4cc2cfee265c248a39eafa8ea6"
   },
   "outputs": [],
   "source": [
    "class_rates = np.array([.81, .82, .79, .79, .85])\n",
    "features = np.array(['SVC', 'RFC', 'AdaBoost', 'GBC', 'DNN'])\n",
    "\n",
    "plt.title('Tuned Model Accuracy Rates')\n",
    "plt.barh(range(len(class_rates)), class_rates, color='b', align='center')\n",
    "plt.yticks(range(len(class_rates)), features) \n",
    "plt.xlabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82126a5b-dcd8-4833-9484-160ece23010a",
    "_uuid": "b392285e6225c1ba1dbd8fb7d338a9c32abcb962"
   },
   "source": [
    "Now that we have selected the best models, we will create a 10 fold cross-validation set. This will allow us to fit our best models (with the parameters selected above) with 9 of the folds and make predictions about the remainig fold. We will cycle through all of the folds and store all of the predictions in an array called 'predictions'. These prediction will be used to begin exploring ways to combine all of our best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "16add719-dcf2-4bd1-97d4-ad21227a0ed3",
    "_uuid": "62e28cfc88d16a5dbece1b16ab5e51029930e581"
   },
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column('x', shape=[18])]\n",
    "skf = StratifiedKFold(n_splits = 10, shuffle = True)\n",
    "j = 0\n",
    "\n",
    "#create a numpy array with the CV predictions for the training data\n",
    "predictions = np.zeros((len(data), 5))\n",
    "\n",
    "#fit all of the models with the training data and fill and array with the predictions for the test data\n",
    "for train_index, test_index in skf.split(data, targets):\n",
    "    X_train = data.iloc[train_index]\n",
    "    X_test = data.iloc[test_index]\n",
    "    y_train = targets.iloc[train_index]\n",
    "    y_test = targets.iloc[test_index]\n",
    "\n",
    "    rf = RandomForestClassifier(max_depth = 9, n_estimators = 300)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    sv = SVC(C=100, gamma = .005, kernel = 'rbf')\n",
    "    sv.fit(X_train, y_train)\n",
    "\n",
    "    ada = AdaBoostClassifier(algorithm = 'SAMME', learning_rate = .5, n_estimators = 100)\n",
    "    ada.fit(X_train, y_train)\n",
    "    \n",
    "    params={'n_estimators': 1000, 'max_depth': 12,\n",
    "            'min_samples_leaf': 5, 'learning_rate': 0.001,\n",
    "            'max_features': 'sqrt', 'random_state': seed}\n",
    "    \n",
    "    gbcm = GradientBoostingClassifier(**params).fit(X_train, y_train)\n",
    "    \n",
    "    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                                  hidden_units=[12, 3],\n",
    "                                                  n_classes=2,\n",
    "                                                  optimizer=tf.train.AdamOptimizer())\n",
    "\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "              x={\"x\": np.array(X_train)},\n",
    "              y=np.array(y_train),\n",
    "              num_epochs=None,\n",
    "              shuffle=True)\n",
    "\n",
    "    classifier.train(input_fn=train_input_fn, steps=5000)\n",
    "\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": np.array(X_test)},\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "\n",
    "    kaanPredictions = list(classifier.predict(input_fn=predict_input_fn))\n",
    "    kaanPredictions=np.array(kaanPredictions)\n",
    "    finalPreds = []\n",
    "    for i in range(len(kaanPredictions)):\n",
    "        temp = kaanPredictions[i]['class_ids'][0]\n",
    "        finalPreds.append(temp)\n",
    "\n",
    "\n",
    "    rf_test = rf.predict(X_test)\n",
    "    sv_test = sv.predict(X_test)\n",
    "    ada_test = ada.predict(X_test)\n",
    "    gbcm_test = gbcm.predict(X_test)\n",
    "\n",
    "    y_true = y_test\n",
    "\n",
    "    for k in range(0, len(test_index)):\n",
    "        predictions[test_index[k]][0] = rf_test[k]\n",
    "        predictions[test_index[k]][1] = sv_test[k]\n",
    "        predictions[test_index[k]][2] = ada_test[k]\n",
    "        predictions[test_index[k]][3] = gbcm_test[k]\n",
    "        predictions[test_index[k]][4] = kaanPredictions[k]['class_ids'][0]\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3fe8c979-0c84-4685-bd1d-a3498bfa3e09",
    "_uuid": "d8460de92e51ca062e412a836fd4b152d0a95505"
   },
   "source": [
    "### Stacked Ensemble Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd46ae30-7af3-49ed-b58f-c2b34ef8e68e",
    "_uuid": "4aeccbad802fca1ea5d1944b7e612b1538439056"
   },
   "source": [
    "This learner will use logestic regression to figure out how to combine the models. This will allow the model to figure out where the different models work well, and where they are weakest. To tune the parameters we experimented with the penalty type and the regularization strength. We will use a 10 fold CV set to find the approximate accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "66d03726-eb7c-4fcf-88cc-5c70b9617015",
    "_uuid": "6496e461103f18f54372472d6554575049b78958",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stacked ensemble model with best SV, RF, LOGREG, ADA, and DNN model\n",
    "acc = np.zeros(10)\n",
    "j = 0\n",
    "for train_index, test_index in skf.split(predictions, targets):\n",
    "    logreg_stack = linear_model.LogisticRegression(penalty = 'l2', C=1e2)\n",
    "    logreg_stack.fit(predictions[train_index], targets[train_index])\n",
    "    \n",
    "    y_true, y_pred = targets[test_index], logreg_stack.predict(predictions[test_index])\n",
    "    \n",
    "    print('\\nFold {} Classification Report'.format(j+1))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    acc[j] = 1.0 * sum(y_pred == y_true) / len(test_index)\n",
    "    j += 1\n",
    "\n",
    "print('\\nAverage Accuracy of the model:')\n",
    "print(mean(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bdfae533-e53d-44c1-a528-0777f6cff41b",
    "_uuid": "b44d566eb88080876ae51491174d7d1de37130f4"
   },
   "source": [
    "### 'Majority Vote' Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2f006ae2-9935-4be0-b1c5-2bbefc32bd5b",
    "_uuid": "c7d7f60fee9ac4e0070ddeec9e90309809802fa1"
   },
   "source": [
    "We will also consider a 'majority vote' ensemble. This learner will use the predictions of the best models to predict whether a passenger will surive or not. The model simply assigns each passenger the prediction the majority of the models have assigned it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ee33ec79-05e6-4331-9232-5ab97657c6d4",
    "_uuid": "208a22c10d18e0efb9f7fcc64414136bb4e5d371"
   },
   "outputs": [],
   "source": [
    "sum(predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9231e70e-d080-4a53-8179-bb4f763b0414",
    "_uuid": "bfda491ce3ad7d6275a5eec79fb9a792c951782d"
   },
   "outputs": [],
   "source": [
    "# majority vote ensemble with best models\n",
    "\n",
    "maj = np.zeros(len(predictions))\n",
    "\n",
    "for i in range(0, len(maj)):\n",
    "    maj[i] = sum(predictions[i])\n",
    "    \n",
    "maj[maj < 3] = 0\n",
    "maj[maj >= 3] = 1  \n",
    "\n",
    "print('\\nClassification Report')\n",
    "print(classification_report(targets, maj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b0d15b72-90ad-4647-8906-6d53bed49d8d",
    "_uuid": "e8aeeebab2d287c85b2b2e5e770d83997f44adb9"
   },
   "source": [
    "# Preprocessing of Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d589044f-fa74-436b-a0d7-a29369f2e87b",
    "_uuid": "c881b1ca37061fa73c2a272fd6bb6cee71c4df8c"
   },
   "source": [
    "We will follow the same preprocessing procedures as the training data for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5b1704b9-21ef-4af1-ab44-d8e3afa9e8fb",
    "_uuid": "5022f11e7b7a7def7362c644a314c8486e95cf4d"
   },
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c5457112-aab3-4ea0-af72-a6c4f1aea44d",
    "_uuid": "6c443e1859f3168155bd335ad3d87b2f31c796a8"
   },
   "source": [
    "### Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c08509b1-d993-4669-8681-8500919b82d7",
    "_uuid": "0596f5b612f33861b237722706e7369507364e68"
   },
   "source": [
    "There was one missing values in Fare for the test data, so we used the median of all the Fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09ddf44d-5a4e-4ffc-b59e-c655b1c15963",
    "_uuid": "26993c7795ec641e06c859de851aea6272d3ed35"
   },
   "outputs": [],
   "source": [
    "test_fare = imputer.fit_transform(test_data[['Fare']])\n",
    "fares_test = pd.DataFrame(test_fare, columns=['Fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "475b98cc-7197-423c-9f32-c1462ab7677e",
    "_uuid": "6b55ce7c07041f90e5cc3979d304552526e50eeb"
   },
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c92e37d2-da76-440f-ac3c-8dc2cc65608d",
    "_uuid": "f69d143f2960e8c93372a64dd39cc15b8b1aa137"
   },
   "outputs": [],
   "source": [
    "test_data['Title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de9ce46f-b393-43b5-845c-0def98d41b55",
    "_uuid": "d432ab487348d7f3d8ec3943266f5434701a1020"
   },
   "source": [
    "We will follow the same basic procedure. This time there are significantly fewer Titles, so only a few need to be mapped to 'Sir' and 'Lady'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "92d70f63-0db8-4509-92da-934b499dd94a",
    "_uuid": "4af60dc6a5d353901b5290efc66ea139d3d9ad37"
   },
   "outputs": [],
   "source": [
    "test_data['Title'] = test_data.Name.map(prefix)\n",
    "test_data['Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7afd1e2f-413e-49b5-9b79-032485231d33",
    "_uuid": "352c51d070e2a65400888147df9ee33b79781a34"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_data.loc[(df_test.Title == 'Rev') |\n",
    "            (df_test.Title == 'Dr') |\n",
    "            (df_test.Title == 'Col'), 'Title']='Sir'\n",
    "\n",
    "test_data.loc[(df_test.Title == 'Dona'), 'Title']='Lady'\n",
    "\n",
    "title_test = lb.fit_transform(test_data['Title'])\n",
    "title_columns_test = lb.classes_\n",
    "titles_test = pd.DataFrame(data=title_test, columns=title_columns_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cccd6f26-96f9-446b-a0a4-bd4d76755105",
    "_uuid": "a978310aba62303153662457abb55297645eca36"
   },
   "source": [
    "### Family Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c2ead077-d998-41d7-adf6-fd5e2558e11d",
    "_uuid": "c60942b4e39d1fd481f21b60213e53612a5d1b2d"
   },
   "outputs": [],
   "source": [
    "test_data['Family_size'] = test_data.SibSp + test_data.Parch + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "240c8d89-41be-41cc-9c4c-6b21b4afeba6",
    "_uuid": "f1dcfd16cd39b6303676dea07054a7ed7d9bf7bc"
   },
   "source": [
    "### Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cce18b57-4a30-470a-8f22-162109bacc45",
    "_uuid": "e5cfee7aa22f573b137c343fb6f314af08c54002"
   },
   "outputs": [],
   "source": [
    "pclass_test = lb.fit_transform(test_data['Pclass'])\n",
    "pclass_columns_test = ['Class1', 'Class2', 'Class3']\n",
    "pclasses_test = pd.DataFrame(data=pclass_test, columns=pclass_columns_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e46cec29-3342-418e-b954-051d0bd598ba",
    "_uuid": "23b331d83c1f906e1db5edf493c284879ba7738d"
   },
   "source": [
    "### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4c8294e9-b1f3-42c1-a585-9f6867482eae",
    "_uuid": "d2bdcb94849ba858ed734f484af1d04fd2df2ebc"
   },
   "outputs": [],
   "source": [
    "sex_test = lb.fit_transform(test_data['Sex'])\n",
    "genders_test = pd.DataFrame(data=sex_test, columns=['Sex_transform'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9a3d98a4-3ae5-402d-b5d4-3c9991ac09e4",
    "_uuid": "61c1471bd0a8f9bd1dbe8d01bfecb3bf969920ef"
   },
   "source": [
    "### Embark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7a237b4d-925c-4ce5-807a-b94947bd773b",
    "_uuid": "f6b9a45f554d1622fcce559a366a45846ee0e98d"
   },
   "source": [
    "There are no missing 'Embarked' values in this data set, so we will just need to convert it to a one-hot encoded dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "34876dd9-936c-44bf-9ba1-df90ffa2f509",
    "_uuid": "fa26a3bcc635fcd7017e3412cce6c479119988ea"
   },
   "outputs": [],
   "source": [
    "embark_test = lb.fit_transform(test_data['Embarked'])\n",
    "embark_columns = lb.classes_\n",
    "embarked_test = pd.DataFrame(data=embark_test, columns=embark_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "83dd0387-f69e-483c-ab00-1f9ba94aacfb",
    "_uuid": "7f1308297989555e8d566ccd7a3aa8f148247ac0"
   },
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6bceea5f-c7f9-4213-b93d-11f74e3c9d4d",
    "_uuid": "80f27656fd88d48e4fae2354814027acf918f8a1"
   },
   "source": [
    "The training and testing datasets were combined to compute the median age values grouped by class and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eea6a6f6-7361-4a12-9336-d44ae5222e54",
    "_uuid": "968a5795c6b14995394dad9c7bdcc348c949c87d"
   },
   "outputs": [],
   "source": [
    "test_data.groupby(['Pclass','Title'])['Age'].median()\n",
    "training_data.groupby(['Pclass','Title'])['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db1979b3-ff56-4e0e-bf94-7dc95b0bd26f",
    "_uuid": "5463356c1170eadb123d276844b09e3873687da4"
   },
   "outputs": [],
   "source": [
    "group_train_test_age = pd.concat([training_data[['Age','Pclass', 'Title']], test_data[['Age','Pclass', 'Title']]], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "13185522-a932-4621-8a0e-d74a3f89eddf",
    "_uuid": "762a7a3393c347897c29af59cdd4181f907c3a64"
   },
   "outputs": [],
   "source": [
    "group_train_test_age.groupby(['Pclass','Title'])['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "675af9d9-f8d7-4bbf-b540-f2fe382c0e9c",
    "_uuid": "c2075b1cb8bf9cfbb5d68cb353dc640d5d989a00"
   },
   "outputs": [],
   "source": [
    "median_age = group_train_test_age.median()\n",
    "test_data.loc[(test_data.Title == 'Ms'), 'Age'] = median_age[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b25ec897-111f-4224-8e1c-80e1c4dc373c",
    "_uuid": "cb192414acc8bc0696c18773df9d9b4c62ed4d47"
   },
   "outputs": [],
   "source": [
    "test_data['Age'] = group_train_test_age.groupby(['Pclass','Title'])['Age'].transform(lambda x:x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eda95ab1-2309-48b2-8d28-c12a0c7ee211",
    "_uuid": "26a152837fd4dd302774026a38e4c2f92093512f"
   },
   "source": [
    "### Combine Transformed Dataframes into Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1f7c26be-8451-4d68-8659-0d7b2bc4715b",
    "_uuid": "1b560e5e02ed9470c6cd6386400f91a54ed0b9a2"
   },
   "outputs": [],
   "source": [
    "numerical_attributes=['SibSp', 'Parch', 'Age', 'Family_size']\n",
    "data_test = pd.concat([test_data[numerical_attributes], fares_test, embarked_test, titles_test, pclasses_test, genders_test], axis=1)\n",
    "numerical_attributes=['SibSp', 'Parch', 'Age', 'Family_size', 'Fare']\n",
    "# scale the numerical attributes\n",
    "data_test[numerical_attributes] = scaler.fit_transform(data_test[numerical_attributes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3fb928d0-3351-4c1f-88bd-a1f2e69730be",
    "_uuid": "c2d01be5d5014594245d4343738901e7856d596c"
   },
   "outputs": [],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37e6d28f-0da4-4832-8b32-b24f088915c2",
    "_uuid": "2db891921d70005aa48af66670ddc2631c4ca228"
   },
   "source": [
    "# Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b2d084a9-ee2a-4664-8159-ea59e7aefccd",
    "_uuid": "3f999ac94de15f50ac1eb9ab04160d583740823c"
   },
   "outputs": [],
   "source": [
    "data_test.drop('Ms', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "210e5b07-5b89-4aa0-add2-32d1b2d6a83b",
    "_uuid": "d097b12c0b643ff22a8d968ccd6f8c16f6cfa6b6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "rf.fit(data, targets)\n",
    "\n",
    "sv.fit(data, targets)\n",
    "\n",
    "ada.fit(data, targets)\n",
    "     \n",
    "gbcm.fit(data, targets)\n",
    "    \n",
    "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                                  hidden_units=[12, 3],\n",
    "                                                  n_classes=2,\n",
    "                                                  optimizer=tf.train.AdamOptimizer())\n",
    "    \n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "              x={\"x\": np.array(data)},\n",
    "              y=np.array(targets),\n",
    "              num_epochs=None,\n",
    "              shuffle=True)\n",
    "\n",
    "classifier.train(input_fn=train_input_fn, steps=10000)\n",
    "\n",
    "\n",
    "# Obtain predictions on the test data\n",
    "rf_test = rf.predict(data_test)\n",
    "sv_test = sv.predict(data_test)\n",
    "ada_test = ada.predict(data_test)\n",
    "gbcm_test = gbcm.predict(data_test)\n",
    "\n",
    "\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": np.array(data_test)},\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "\n",
    "dnnPredictions = list(classifier.predict(input_fn=predict_input_fn))\n",
    "dnnPredictions=np.array(dnnPredictions)\n",
    "finalDnnPreds = []\n",
    "for i in range(len(dnnPredictions)):\n",
    "    temp = dnnnPredictions[i]['class_ids'][0]\n",
    "    finalDnnPreds.append(temp)\n",
    "\n",
    "model_predictions = np.zeros((len(data_test), 5))\n",
    "\n",
    "model_predictions[:,0] = rf_test\n",
    "model_predictions[:,1] = sv_test\n",
    "model_predictions[:,2] = gbcm_test\n",
    "model_predictions[:,3] = ada_test\n",
    "model_predictions[:,4] = finalDnnPreds\n",
    "\n",
    "#use the stack_model fitted with all of the training data\n",
    "logreg_final = linear_model.LogisticRegression(penalty = 'l2', C=1e2)\n",
    "logreg_final.fit(predictions, targets)\n",
    "\n",
    "test_preds = logreg_final.predict(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b0d70c8-38f7-45c1-b047-29751f03cdfb",
    "_uuid": "6b023c542c77eea84fe3fe2ca7b082fe976667d8"
   },
   "outputs": [],
   "source": [
    "#export the data\n",
    "submission = pd.DataFrame({'PassengerId': df_test['PassengerId'] , 'Survived': test_preds})\n",
    "submission.to_csv('titanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37f6053c-ff84-494f-8c02-bcecee0c0adb",
    "_uuid": "d6fecc82b7faf5c3b66d6f62e4990339edbb097f",
    "collapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f822b0d2-c9d5-408b-8663-c9fe0a14f143",
    "_uuid": "3bd04ab1737c2a37c1517c6cc59f70c4fc0e1321"
   },
   "source": [
    "Our best model ended up with an accuracy of .78947. To improve this score even further we would devote some more time to feature extraction. In our model we didn't include the predictors 'Ticket' or 'Cabin'. When we examined Ticket, there were 681 unique values out of the 891 entries. Since the vast majority of passengers had a unique ticket number we didn't see any value in including it in our models. The 'Cabin' predictor could have been very useful. Passengers closer to stairs or the lifeboats might have an increased rate of survival. We were unable to use it as a predictor because of all of the missing values in the data. It might be worth while to see if we are able to use the information in the ticket column to assign each passenger to a cabin.\n",
    "<br>\n",
    "<br>\n",
    "There were several challenges we encountered while working on this project. One particularly vexing challenge was handling the missing values in the data. The training set and test set were both missing large amounts of the 'Age' feature. We tried to impute these 'Age' values with a predictive neural net model trained on the other features, but problems with importing these values back into the original pandas DataFrame prevented us from fully realizing this imputation. We decided to instead impute \n",
    "these values with the median value of both the training and test set combined. \n",
    "<br>\n",
    "<br>\n",
    "Another challenge/limitation was the use of black-box API calls to run our learning algorithms. For instance, our neural net classifier was built using the tensorflow.estimator.DNNClassifier class, which automatically creates a neural net scoped with user parameters. This offered ease of implementation at the possible cost of greater control over the model. We encountered the same limitation when using the scikit-learn API to build our other models, although this was somewhat mitigated by our usage of hyperparameter optimization. We trained our neural net model 289 times with different combinations of hidden layer width to determine which would give us the best performance. We also used grid search to tune the parameters that we passed to our SVM, random forest, GBC, and Adaboosting classifiers. \n",
    "<br>\n",
    "<br>\n",
    "Finally, our team felt that the data set was rather limited in the sense that of the 2200 actual passengers on board, we only had 891 training values, with many missing feature values for age. We understand that data in real-world, practical situations often has similar limitations, but that does not make the task before us any easier. We also believe that leaving out crew member data (we think that's the case), leads to some loss in predictive power. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
